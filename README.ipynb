{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Kronicle — Quickstart / Integration Test\n",
    "\n",
    "This notebook demonstrates how to use the `kronicle-sdk` to:\n",
    "\n",
    "1. Create or upsert a channel via **KronicleWriter**\n",
    "2. Insert rows of timeseries data\n",
    "3. Read the channel metadata with **KronicleReader**\n",
    "4. Read the rows back (as dict or Pandas DataFrame)\n",
    "5. Perform simple verification assertions\n",
    "\n",
    "Every created channel automatically receives a tag:\n",
    "\n",
    "```\n",
    "test: <now_utc>\n",
    "```\n",
    "\n",
    "This helps isolate test runs and later delete all test-tagged channels using KronicleSetup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Adjust the URL if your Kronicletorage API runs elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.iso_datetime import now_local\n",
    "from utils.str_utils import tiny_id, uuid4_str\n",
    "\n",
    "BASE_URL = \"http://localhost:8000\"  # Adjust if needed. The Kronicle server should of course be running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Instantiate a Writer connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from connectors.kronicle_writer import KronicleWriter\n",
    "\n",
    "writer = KronicleWriter(BASE_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Scan the Kronicle server with the read abilities of the KronicleWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_channel_id, max_row_nb = writer.get_channel_with_max_rows()\n",
    "print(\"Higher count of rows is\",max_row_nb,\"for channel\", main_channel_id)\n",
    "main_channel = writer.get_channel(main_channel_id)\n",
    "print(writer.get_channel(main_channel_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Review the data for this channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data as rows\n",
    "writer.get_rows_for_channel(main_channel_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data as columns\n",
    "writer.get_cols_for_channel(main_channel_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Prepare a unique test channel\n",
    "\n",
    "We generate a fresh `sensor_id` for each run.\n",
    "\n",
    "A unique **test tag** is added to the channel automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_id = uuid4_str()\n",
    "sensor_name = f\"demo_channel_{tiny_id()}\"\n",
    "now_tag = now_local()\n",
    "\n",
    "payload = {\n",
    "    \"sensor_id\": sensor_id,\n",
    "    \"sensor_name\": sensor_name,\n",
    "    \"sensor_schema\": {\"time\": \"datetime\", \"temperature\": \"float\"},\n",
    "    \"metadata\": {\"unit\": \"°C\"},\n",
    "    \"tags\": {\"test\": now_tag},\n",
    "    \"rows\": [\n",
    "        {\"time\": \"2025-01-01T00:00:00Z\", \"temperature\": 12.3},\n",
    "        {\"time\": \"2025-01-01T00:01:00Z\", \"temperature\": 12.8},\n",
    "    ],\n",
    "}\n",
    "\n",
    "payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Create/Upsert the Channel + Insert Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('payload:', payload)\n",
    "result = writer.insert_rows_and_upsert_channel(payload)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Check the stored Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = writer.get_channel(sensor_id)\n",
    "channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Read Rows (DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = writer.get_rows_for_channel(sensor_id, return_type=\"df\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Basic Assertions\n",
    "\n",
    "These confirm both the SDK and the backend behaved as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df) == 2, \"Should have exactly 2 rows\"\n",
    "assert abs(df[\"temperature\"].iloc[0] - 12.3) < 1e-9\n",
    "assert abs(df[\"temperature\"].iloc[1] - 12.8) < 1e-9\n",
    "\n",
    "print(\"✔ Basic read/write integration test succeeded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from kronicle.models.iso_datetime import IsoDateTime, now\n",
    "from kronicle.models.kronicable_sample import KronicableSample\n",
    "from kronicle.models.kronicable_type import KronicableTypeChecker\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Nested BaseModel example\n",
    "# ------------------------------------------------------------\n",
    "class MetaData(BaseModel):\n",
    "    unit: str\n",
    "    description: Optional[str] = None  # Optional field inside nested BaseModel\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main KronicableSample with optional primitives, nested BaseModels, list and dict\n",
    "# ------------------------------------------------------------\n",
    "class SensorSample(KronicableSample):\n",
    "    timestamp: IsoDateTime\n",
    "    temperature: Optional[float] = None            # Optional primitive\n",
    "    meta: Optional[MetaData] = None                # Optional nested BaseModel\n",
    "    tags: Optional[list[str]] = None               # Optional list of primitives\n",
    "    extra: Optional[dict[str, MetaData]] = None    # Optional dict of BaseModels\n",
    "    test_field: float | None = None\n",
    "    test_meta: MetaData | None = None\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create sample instance with partial data\n",
    "# ------------------------------------------------------------\n",
    "sample = SensorSample(\n",
    "    timestamp=now(),\n",
    "    temperature=23.5,\n",
    "    meta=MetaData(unit=\"°C\"),                     # nested optional BaseModel\n",
    "    tags=[\"room1\", \"test\"],                       # optional list of primitives\n",
    "    extra={\"sensor1\": MetaData(unit=\"°C\", description=\"backup\")}  # optional dict of BaseModels\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Verify that KronicableTypeChecker correctly identifies optional fields\n",
    "# ------------------------------------------------------------\n",
    "for name, field in sample.model_fields.items():\n",
    "    kt = KronicableTypeChecker(field.annotation)\n",
    "    print(f\"Field '{name}': valid={kt.is_valid()}, optional={kt.is_optional()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Convert the sample to a dictionary suitable for KroniclePayload\n",
    "# Nested BaseModels, lists, and dicts should be serialized to JSON strings\n",
    "# ------------------------------------------------------------\n",
    "row_dict = sample.to_row()\n",
    "print(\"\\nSerialized row dictionary:\")\n",
    "print(row_dict)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Inspect the sensor schema generated by get_sensor_schema()\n",
    "# Optional fields should appear as optional[...] in the schema\n",
    "# ------------------------------------------------------------\n",
    "schema = sample.get_sensor_schema()\n",
    "print(\"\\nGenerated sensor schema:\")\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Proceed to Cleanup (via KronicleSetup)\n",
    "\n",
    "Not implemented yet in this notebook — once `KronicleSetup` is ready, you will be able to automatically delete all channels containing the `test:` tag.\n",
    "\n",
    "Example snippet to add later:\n",
    "```python\n",
    "# setup = KronicleSetup(BASE_URL)\n",
    "# setup.delete_channels_by_tag(\"test\", now_tag)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
